{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use(\"ggplot\") # 這行程式碼設置了 matplotlib 的繪圖風格。\n",
    "\n",
    "# 這是 Jupyter Notebook 的魔法命令，它用於指定 matplotlib 繪圖的輸出方式。\n",
    "# 可以直接在 Notebook 中看到圖形，而不需要使用 plt.show()。\n",
    "%matplotlib inline \n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "import random \n",
    "\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, path, ids, batch_size=8, image_size=640):\n",
    "        self.path = path\n",
    "        self.ids = ids\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "    # 回傳 steps_per_epoch , 也就是一個 epoch 需要幾個 batch \n",
    "    def __len__(self): \n",
    "        return int(np.ceil(len(self.ids) / self.batch_size))\n",
    "        \n",
    "    def __load__(self, id_name):\n",
    "        image_path = os.path.join(self.path, \"Images\", id_name)\n",
    "        mask_path = os.path.join(self.path, \"Mask\", id_name)\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "\n",
    "        # Normalizaing \n",
    "        image = image / 255.0\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    # 獲取當次 batch 要訓練的照片\n",
    "    def __getitem__(self, index):\n",
    "        if (index + 1) * self.batch_size > len(self.ids):\n",
    "            ids_batch = self.ids[index * self.batch_size : len(self.ids)]\n",
    "        else:\n",
    "            ids_batch = self.ids[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        \n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        for id_name in ids_batch:\n",
    "            _img, _mask = self.__load__(id_name)\n",
    "            images.append(_img)\n",
    "            masks.append(_mask)\n",
    "        \n",
    "        images = np.array(images)\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        return images, masks\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "(2, 640, 640, 1) (2, 640, 640, 1)\n",
      "(1, 640, 640, 1) (1, 640, 640, 1)\n",
      "(0,) (0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24445"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 參數設定\n",
    "train_path = \"dataset_copy_3_imgs/train/\"\n",
    "train_ids = os.listdir(train_path + \"Images\") # list\n",
    "valid_path = \"dataset_copy_3_imgs/valid/\"\n",
    "valid_ids = os.listdir(valid_path + \"Images\") # list\n",
    "batch_size = 2\n",
    "image_size = 640\n",
    "\n",
    "# 建立 DataGen 物件\n",
    "train_data = DataGen(train_path, train_ids, batch_size, image_size)\n",
    "valid_data = DataGen(valid_path, valid_ids, batch_size, image_size)\n",
    "\n",
    "# 使用 DataGen 類別的 __len__ 函式\n",
    "print(train_data.__len__())\n",
    "print(valid_data.__len__())\n",
    "\n",
    "# 使用 DataGen 類別的 __getitem__ 函式\n",
    "x, y = train_data.__getitem__(0)\n",
    "print(x.shape, y.shape)\n",
    "x, y = train_data.__getitem__(1)\n",
    "print(x.shape, y.shape)\n",
    "x, y = train_data.__getitem__(2)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "gc.collect() #  用於清理不再使用的對象，釋放內存。（Garbage Collection）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "\n",
    "def conv_batchnorm_relu_block(input_tensor, nb_filter, kernel_size=3):\n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same')(input_tensor)\n",
    "    x = BatchNormalization(axis=2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def UnetPP(input_shape, n_labels, using_deep_supervision=False):\n",
    "\n",
    "    nb_filter = [32,64,128,256,512]\n",
    "\n",
    "    # Set image data format to channels first\n",
    "    global bn_axis\n",
    "\n",
    "    K.set_image_data_format(\"channels_last\")\n",
    "    bn_axis = -1\n",
    "    inputs = Input(shape=input_shape, name='input_image')\n",
    "\n",
    "    conv1_1 = conv_batchnorm_relu_block(inputs, nb_filter=nb_filter[0])\n",
    "    pool1 = AvgPool2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
    "\n",
    "    conv2_1 = conv_batchnorm_relu_block(pool1, nb_filter=nb_filter[1])\n",
    "    pool2 = AvgPool2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
    "\n",
    "    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
    "    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
    "    conv1_2 = conv_batchnorm_relu_block(conv1_2,  nb_filter=nb_filter[0])\n",
    "\n",
    "    conv3_1 = conv_batchnorm_relu_block(pool2, nb_filter=nb_filter[2])\n",
    "    pool3 = AvgPool2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
    "\n",
    "    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
    "    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
    "    conv2_2 = conv_batchnorm_relu_block(conv2_2, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
    "    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
    "    conv1_3 = conv_batchnorm_relu_block(conv1_3, nb_filter=nb_filter[0])\n",
    "\n",
    "    conv4_1 = conv_batchnorm_relu_block(pool3, nb_filter=nb_filter[3])\n",
    "    pool4 = AvgPool2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
    "\n",
    "    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
    "    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
    "    conv3_2 = conv_batchnorm_relu_block(conv3_2, nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
    "    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
    "    conv2_3 = conv_batchnorm_relu_block(conv2_3, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
    "    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
    "    conv1_4 = conv_batchnorm_relu_block(conv1_4, nb_filter=nb_filter[0])\n",
    "\n",
    "    conv5_1 = conv_batchnorm_relu_block(pool4, nb_filter=nb_filter[4])\n",
    "\n",
    "    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
    "    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
    "    conv4_2 = conv_batchnorm_relu_block(conv4_2, nb_filter=nb_filter[3])\n",
    "\n",
    "    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
    "    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
    "    conv3_3 = conv_batchnorm_relu_block(conv3_3, nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
    "    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
    "    conv2_4 = conv_batchnorm_relu_block(conv2_4, nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
    "    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
    "    conv1_5 = conv_batchnorm_relu_block(conv1_5, nb_filter=nb_filter[0])\n",
    "\n",
    "    nestnet_output_1 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_1',padding='same')(conv1_2)\n",
    "    nestnet_output_2 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_2', padding='same' )(conv1_3)\n",
    "    nestnet_output_3 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_3', padding='same')(conv1_4)\n",
    "    nestnet_output_4 = Conv2D(n_labels, (1, 1), activation='sigmoid', name='output_4', padding='same')(conv1_5)\n",
    "\n",
    "    if using_deep_supervision:\n",
    "        model = Model(input=inputs, output=[nestnet_output_1,\n",
    "                                            nestnet_output_2,\n",
    "                                            nestnet_output_3,\n",
    "                                            nestnet_output_4])\n",
    "    else:\n",
    "        model = Model(inputs=inputs, outputs=nestnet_output_4)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# input_img = Input((h, w, 1), name='img')\n",
    "# del model\n",
    "model = UnetPP(input_shape = (640, 640, 1), n_labels=1) # change \n",
    "metrics = [\"accuracy\", \n",
    "           tf.keras.metrics.AUC(), \n",
    "           tf.keras.metrics.SensitivityAtSpecificity(0.5), \n",
    "           tf.keras.metrics.SpecificityAtSensitivity(0.5)]\n",
    "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=metrics)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=7, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model-UnetPP.h5', verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    CSVLogger(\"dataUnetPP.csv\"),\n",
    "    TensorBoard(log_dir='./logs')\n",
    "]\n",
    "# >> tensorboard --logdir=path/to/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入最近的檢查點的權重\n",
    "model.load_weights('model-UnetPP.h5') # change \n",
    "results = model.fit(, batch_size=1, epochs=5, callbacks=callbacks, validation_data=(X_test, y_test), use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
